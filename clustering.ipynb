{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:16.826003Z",
     "start_time": "2019-01-18T02:38:12.907719Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import array, struct, split, explode, udf, col, collect_list\n",
    "from  pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName('haha')\n",
    "sc = SparkContext(conf=conf)\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:46.685836Z",
     "start_time": "2019-01-18T02:38:16.828570Z"
    }
   },
   "outputs": [],
   "source": [
    "review = ss.read.json('Books_review.json')\n",
    "meta = ss.read.json('meta_Books.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:51.717315Z",
     "start_time": "2019-01-18T02:38:46.700835Z"
    }
   },
   "outputs": [],
   "source": [
    "#review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:51.781091Z",
     "start_time": "2019-01-18T02:38:51.721072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'helpful', 'overall', 'reviewText', 'reviewTime', 'reviewerID', 'reviewerName', 'summary', 'unixReviewTime']\n",
      "['_corrupt_record', 'asin', 'brand', 'categories', 'description', 'imUrl', 'price', 'related', 'salesRank', 'title']\n"
     ]
    }
   ],
   "source": [
    "print(review.columns)\n",
    "print(meta.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for review data, we only focus on 2012, 2013, 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:51.968545Z",
     "start_time": "2019-01-18T02:38:51.783411Z"
    }
   },
   "outputs": [],
   "source": [
    "df2012 = review.where(review.reviewTime.contains('2012'))\n",
    "df2013 = review.where(review.reviewTime.contains('2013'))\n",
    "df2014 = review.where(review.reviewTime.contains('2014'))\n",
    "reviews = df2012.union(df2013).union(df2014)   # 6132506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat reviewText and summary together  ==> textss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:52.023511Z",
     "start_time": "2019-01-18T02:38:51.970751Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews = reviews.select('asin',\n",
    "                        concat(col(\"reviewText\"),lit(\"\"),col(\"summary\")).alias('textss'))\n",
    "# here, asin is not distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:53.609032Z",
     "start_time": "2019-01-18T02:38:52.025794Z"
    }
   },
   "outputs": [],
   "source": [
    "#reviews.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for meta data, we only need 4 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:38:53.669711Z",
     "start_time": "2019-01-18T02:38:53.611211Z"
    }
   },
   "outputs": [],
   "source": [
    "meta = meta.drop_duplicates(['asin']).select('asin', 'title','categories', 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:39:14.384528Z",
     "start_time": "2019-01-18T02:38:53.671758Z"
    }
   },
   "outputs": [],
   "source": [
    "#meta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review data group by asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:39:14.427491Z",
     "start_time": "2019-01-18T02:39:14.386798Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_gb = reviews.groupBy(\"asin\").agg(f.concat_ws(\" \", f.collect_list('textss')).alias('textss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join review data and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:40:14.010679Z",
     "start_time": "2019-01-18T02:40:13.574589Z"
    }
   },
   "outputs": [],
   "source": [
    "df = reviews_gb.join(meta, 'asin', 'inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:40:50.214380Z",
     "start_time": "2019-01-18T02:40:14.013095Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat all TEXT info together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:40:50.280439Z",
     "start_time": "2019-01-18T02:40:50.220214Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.select('asin','title','categories',\n",
    "                concat(col(\"textss\"),lit(\"\"),col(\"description\")).alias('reviewText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:02.713733Z",
     "start_time": "2019-01-18T02:40:50.283817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83946"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('asin').distinct().count()  # asin is distinct in this dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:02.809478Z",
     "start_time": "2019-01-18T02:41:02.716334Z"
    }
   },
   "outputs": [],
   "source": [
    "#~df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:02.836492Z",
     "start_time": "2019-01-18T02:41:02.811781Z"
    }
   },
   "outputs": [],
   "source": [
    "dff = df.na.drop(subset=[\"reviewText\"])  # drop nulls /  from 277342 to 169873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:02.863154Z",
     "start_time": "2019-01-18T02:41:02.838666Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "dff = dff.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINALLY, get the nice table we want. ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:02.978622Z",
     "start_time": "2019-01-18T02:41:02.864891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+---+\n",
      "|      asin|               title|categories|          reviewText| id|\n",
      "+----------+--------------------+----------+--------------------+---+\n",
      "|1481916025|Acquiring Trouble...| [[Books]]|Awesome!  Does ev...|  0|\n",
      "|1481948725|The Shadow Editio...| [[Books]]|I'll say one thin...|  1|\n",
      "|1482336251|         Sharp Edges| [[Books]]|There were a few ...|  2|\n",
      "|1482571145|Pull: A Seaside N...| [[Books]]|Amazing!  I loved...|  3|\n",
      "|1482635720|    The Kona Shuffle| [[Books]]|This was an inter...|  4|\n",
      "+----------+--------------------+----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:44:23.129471Z",
     "start_time": "2019-01-18T02:44:21.769998Z"
    }
   },
   "outputs": [],
   "source": [
    "#dff.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:41:03.567353Z",
     "start_time": "2019-01-18T02:41:02.982128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+\n",
      "|      asin|               title|categories|          reviewText| id|               words|            mf_words|\n",
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+\n",
      "|1481916025|Acquiring Trouble...| [[Books]]|Awesome!  Does ev...|  0|[awesome!, , does...|[awesome!, , ever...|\n",
      "|1481948725|The Shadow Editio...| [[Books]]|I'll say one thin...|  1|[i'll, say, one, ...|[say, one, thing,...|\n",
      "|1482336251|         Sharp Edges| [[Books]]|There were a few ...|  2|[there, were, a, ...|[twist, &turns;, ...|\n",
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\") #other languages+capital letters?\n",
    "df_w_words = tokenizer.transform(dff)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"mf_words\")  # need to add in \"book\"\n",
    "df_w_mfwords = remover.transform(df_w_words)\n",
    "df_w_mfwords.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:45:01.337566Z",
     "start_time": "2019-01-18T02:44:34.624922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|               title|categories|          reviewText| id|               words|            mf_words|                  tf|               tfidf|\n",
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|1481916025|Acquiring Trouble...| [[Books]]|Awesome!  Does ev...|  0|[awesome!, , does...|[awesome!, , ever...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|\n",
      "|1481948725|The Shadow Editio...| [[Books]]|I'll say one thin...|  1|[i'll, say, one, ...|[say, one, thing,...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|\n",
      "|1482336251|         Sharp Edges| [[Books]]|There were a few ...|  2|[there, were, a, ...|[twist, &turns;, ...|(200,[0,1,2,3,4,5...|(200,[0,1,2,3,4,5...|\n",
      "+----------+--------------------+----------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"mf_words\", outputCol=\"tf\", numFeatures=200) #need to know how to choose the numFeatures\n",
    "tf = hashingTF.transform(df_w_mfwords)  # reviews_w_feature  == tf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:45:35.327866Z",
     "start_time": "2019-01-18T02:45:01.341020Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf.select('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vecs = tfidf.select('tfidf').rdd.map(lambda x: x[0]).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:47:08.845259Z",
     "start_time": "2019-01-18T02:45:55.346085Z"
    }
   },
   "outputs": [],
   "source": [
    "#full_matrix=[]\n",
    "#for i in range(len(feature_vecs)):\n",
    "#    fea_vec=[]\n",
    "#    for i in range(len(feature_vecs[0])):\n",
    "#        temp = feature_vecs[0][i]\n",
    "#        fea_vec.append(temp)\n",
    "#    full_matrix.append(fea_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T02:47:26.086488Z",
     "start_time": "2019-01-18T02:47:24.129977Z"
    }
   },
   "outputs": [],
   "source": [
    "#full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features_matrix_rdd = sc.parallelize(full_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features_matrix_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features_matrix_df = ss.createDataFrame(Features_matrix_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-77f6a2670034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"normFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ml2NormData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLDAModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\")\n",
    "l2NormData = normalizer.transform(feature_vecs)\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, LDA, LDAModel\n",
    "\n",
    "kmeans = KMeans().setK(10).setMaxIter(20)\n",
    "km_model = kmeans.fit(l2NormData)\n",
    "clustersTable = km_model.transform(l2NormData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = km_model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersTable.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=2, seed=1, optimizer=\"em\")\n",
    "model = lda.fit(l2NormData)\n",
    "model.describeTopics().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = temp_path + \"/lda\"\n",
    "lda.save(lda_path)\n",
    "sameLDA = LDA.load(lda_path)\n",
    "distributed_model_path = temp_path + \"/lda_distributed_model\"\n",
    "model.save(distributed_model_path)\n",
    "sameModel = DistributedLDAModel.load(distributed_model_path)\n",
    "local_model_path = temp_path + \"/lda_local_model\"\n",
    "localModel.save(local_model_path)\n",
    "sameLocalModel = LocalLDAModel.load(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
