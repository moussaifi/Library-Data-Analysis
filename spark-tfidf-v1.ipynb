{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T03:32:09.980697Z",
     "start_time": "2019-01-17T03:31:59.521754Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import array, struct, split, explode, udf, col, collect_list\n",
    "from  pyspark.sql import functions as f\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName('haha')\n",
    "sc = SparkContext(conf=conf)\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T03:36:49.699001Z",
     "start_time": "2019-01-17T03:36:45.792350Z"
    }
   },
   "outputs": [],
   "source": [
    "review_sp = ss.read.json('/Users/Reagan/Downloads/xaa.json').select('asin','reviewText')\n",
    "reviews = review_sp.groupBy(\"asin\").agg(f.concat_ws(\" \", f.collect_list('reviewText')).alias('reviewText'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T03:36:59.396737Z",
     "start_time": "2019-01-17T03:36:49.701551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|      asin|          reviewText|               words|            mf_words|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|0001837192|At least it was f...|[at, least, it, w...|[least, first, re...|\n",
      "|0001845357|Last year I track...|[last, year, i, t...|[last, year, trac...|\n",
      "|0002216973|And to me, there'...|[and, to, me,, th...|[me,, big, differ...|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "reviews_w_words = tokenizer.transform(reviews)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"mf_words\")  # need to add in \"book\"\n",
    "reviews_w_mfwords = remover.transform(reviews_w_words)\n",
    "reviews_w_mfwords.show(3)\n",
    "# need improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T04:31:56.372288Z",
     "start_time": "2019-01-17T04:31:48.913972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+---+\n",
      "|      asin|          reviewText|               words|            mf_words| id|\n",
      "+----------+--------------------+--------------------+--------------------+---+\n",
      "|0001837192|At least it was f...|[at, least, it, w...|[least, first, re...|  0|\n",
      "|0001845357|Last year I track...|[last, year, i, t...|[last, year, trac...|  1|\n",
      "|0002216973|And to me, there'...|[and, to, me,, th...|[me,, big, differ...|  2|\n",
      "+----------+--------------------+--------------------+--------------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "df_index = reviews_w_mfwords.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "df_index.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T04:12:49.938308Z",
     "start_time": "2019-01-17T04:11:49.024916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      asin|          reviewText|               words|            mf_words|                  tf|               tfidf|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|0001837192|At least it was f...|[at, least, it, w...|[least, first, re...|(1000,[8,40,65,77...|(1000,[8,40,65,77...|\n",
      "|0001845357|Last year I track...|[last, year, i, t...|[last, year, trac...|(1000,[1,2,5,9,12...|(1000,[1,2,5,9,12...|\n",
      "|0002216973|And to me, there'...|[and, to, me,, th...|[me,, big, differ...|(1000,[0,4,5,6,7,...|(1000,[0,4,5,6,7,...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"mf_words\", outputCol=\"tf\", numFeatures=1000)\n",
    "tf = hashingTF.transform(reviews_w_mfwords)  # reviews_w_feature  == tf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T05:19:54.891990Z",
     "start_time": "2019-01-17T05:19:54.875045Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf.select('tfidf').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-17T05:20:33.215Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_vecs=tfidf.select('tfidf').rdd.map(lambda x: x[0]).collect()  \n",
    "full_matrix=[]\n",
    "for i in range(len(feature_vecs)):\n",
    "    fea_vec=[]\n",
    "    for i in range(len(feature_vecs[0])):\n",
    "        temp = feature_vecs[0][i]\n",
    "        fea_vec.append(temp)\n",
    "    full_matrix.append(fea_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
